\documentclass[12pt,a4paper]{paper}
\usepackage[utf8]{inputenc}
\usepackage[english]{babel}
\usepackage{amsmath}
\usepackage{enumitem}
\usepackage{fixltx2e}
\usepackage{multicol}
\usepackage{amsmath}
\usepackage{enumitem}
\usepackage{arydshln}
\usepackage{amsfonts}
\usepackage{multirow}
\usepackage{multicol}
\usepackage{amssymb}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage[left=1cm,right=1cm,top=1.5cm,bottom=2cm]{geometry}
\begin{document}
\title{STAT636 - Homework 4\\\small{Daniel Osorio - dcosorioh@tamu.edu\\Department of Veterinary Integrative Biosciences\\Texas A\&M University}}
\maketitle
\SweaveOpts{concordance=TRUE}
\begin{enumerate}
\item Consider the \texttt{wine\_quality\_red} data. Due to privacy and logistic issues, only physicochemical (inputs) and sensory (the output) variables are available (e.g. there is no data about grape types, wine brand, wine selling price, etc.).These contain eleven different variables (1 : fixed acidity; 2 : volatile acidity; 3 : citric acid; 4 : residual sugar; 5 : chlorides; 6 : free sulfur dioxide; 7 : total sulfur dioxide; 8 : density; 9 : pH; 10 : sulphates; 11 : alcohol) and one output (quality (score between 0 and 10) ). Use prcomp to perform principal component analysis.
<<>>=
wqr <- read.csv("winequality_red.csv")
data <- scale(wqr[,-12], center = TRUE, scale = TRUE)
pca_data <- prcomp(data)
@
\begin{enumerate}
\item What proportion of variance is explained by the first principal component? 
<<>>=
summary(pca_data)$importance[2,1]
@
\item[] By the second principal component?
<<>>=
summary(pca_data)$importance[2,2]
@
\item Compute the Proportion of Variance Explained and Cumulative Proportion of Variance Explained and plot the figures. 
<<fig=TRUE, height=2>>=
summary(pca_data)$importance
par(mar=c(3,3,0.5,0.5))
plot(summary(pca_data)$importance[2,], 
     xlab = "PC", 
     ylab = "Proportion of Variance", 
     las=1, type = "l")
@
\item[] How many PCs do you need if you want to capture around 80\% variance?
<<>>=
sum(summary(pca_data)$importance[3,] <= 0.8)
@
\item Verify that the variances of the first two PCs equal the first two eigenvalues of the sample covariance matrix \textbf{S}
<<>>=
S <- var(data)
eS <- eigen(S)
eS$values[1:2]
apply(data %*% eS$vectors,2,var)[1:2]
@
\item Report a scatterplot of the first two principal components and the first with the third principal components.
<<fig=TRUE, height=2>>=
par(mfrow=c(1,2), mar=c(4,4,0.5,0.5))
plot((data %*% eS$vectors)[,c(1,2)], xlab = "PC1", ylab = "PC2", las = 1)
plot((data %*% eS$vectors)[,c(1,3)], xlab = "PC1", ylab = "PC3", las = 1)
@
\item Which number of wine have the minimum and maximum values of PC1?
<<>>=
which.min((data %*% eS$vectors)[,1])
which.max((data %*% eS$vectors)[,1])
@
\end{enumerate}
\item Consider the matrix of distances for four items
\[\left[\begin{array}{cccc}0&&&\\3&0&&\\2&4&0&\\5&1&7&0\end{array}\right]\]
For each of single, complete, and average linkage:
<<>>=
D <- as.dist(matrix(c(0,3,2,5,3,0,4,1,2,4,0,7,5,1,7,0), nrow = 4, byrow = TRUE))
@
\begin{enumerate}
\item List all intermediate distance matrices involved in the hierarchical clustering routine.
\begin{enumerate}
\item Using single linkage:
\[\left[\begin{array}{cccc}0&&&\\3&0&&\\2&4&0&\\5&\textbf{1}&7&0\end{array}\right] \rightarrow
\left[\begin{array}{ccc}0&&\\3&0&\\4&\textbf{2}&0\end{array}\right] \rightarrow
\left[\begin{array}{cc}0&\\\textbf{3}&0\end{array}\right]\]
\item Using complete linkage:
\[\left[\begin{array}{cccc}0&&&\\3&0&&\\2&4&0&\\5&\textbf{1}&7&0\end{array}\right] \rightarrow
\left[\begin{array}{ccc}0&&\\5&0&\\7&\textbf{2}&0\end{array}\right] \rightarrow
\left[\begin{array}{cc}0&\\\textbf{7}&0\end{array}\right]
\]
\item Using average linkage:
\[\left[\begin{array}{cccc}0&&&\\3&0&&\\2&4&0&\\5&\textbf{1}&7&0\end{array}\right] \rightarrow
\left[\begin{array}{ccc}0&&\\4&0&\\5.5&\textbf{2}&0\end{array}\right] \rightarrow
\left[\begin{array}{cc}0&\\\textbf{4.75}&0\end{array}\right]
\]
\end{enumerate}
\item Draw the dendrograms and compare the results of the different linkage methods.
<<fig=TRUE, height=2>>=
par(mfrow=c(1,3), mar=c(1,3,1,1))
plot(hclust(d = D, method = "single"), main = "Single", las=1)
plot(hclust(d = D, method = "complete"), main = "Complete", las=1)
plot(hclust(d = D, method = "average"), main= "Average", las=1)
@
\end{enumerate}
\item Suppose we measure two variables $X_1$ and $X_2$ for four items $A$, $B$, $C$, and $D$. The data are as follows:
\begin{center}
\begin{tabular}{c|cc}
&\multicolumn{2}{c}{Observations}\\
Item&$x_{1}$&$x_{2}$\\
\hline
A&5&4\\
B&1&-2\\
C&-1&1\\
D&3&1\\
\end{tabular}
\end{center}
\begin{enumerate}
\item Starting with the initial groups (AB) and (CD), write out all steps to the K-means clustering routine with K = 2. Use Euclidean distance on the unstandardized variables.
\begin{enumerate}
\item Compute the coordinates of the cluster centroid (mean)\\
\begin{tabular}{ccc}
&$x_{1}$&$x_{2}$\\
(AB)&3&1\\
(CD)&1&1
\end{tabular}
\item Compute the Euclidean distance of each item from the group centroids and reassign each item to the nearest group.\\
\begin{tabular}{ccc}
&$(AB)$&$(CD)$\\
(A)&\textbf{3.6}&5\\
(B)&3.6&\textbf{3}\\
(C)&4&\textbf{2}\\
(D)&\textbf{0}&2\\
\end{tabular}
\item As two items were moved from the initial configuration,the cluster centroids (means) must be updated.\\
\begin{tabular}{ccc}
&$x_{1}$&$x_{2}$\\
(AD)&4&2.5\\
(BC)&0&-0.5
\end{tabular}
\item Compute the Euclidean distance of each item from the group centroids and reassign each item to the nearest group.\\
\begin{tabular}{ccc}
&$(AD)$&$(BC)$\\
(A)&\textbf{1.8}&6.72\\
(B)&5.4&\textbf{1.8}\\
(C)&5.22&\textbf{1.8}\\
(D)&\textbf{1.8}&3.35\\
\end{tabular}
\end{enumerate}
\item Repeat, now starting with the initial groups (AC) and (BD). Compare the results with those in the part (a).
\begin{enumerate}
\item Compute the coordinates of the cluster centroid (mean)\\
\begin{tabular}{ccc}
&$x_{1}$&$x_{2}$\\
(AC)&2&2.5\\
(BD)&2&-0.5
\end{tabular}
\item Compute the Euclidean distance of each item from the group centroids and reassign each item to the nearest group.\\
\begin{tabular}{ccc}
&$(AC)$&$(BD)$\\
(A)&\textbf{3.35}&5.4\\
(B)&4.6&\textbf{1.8}\\
(C)&\textbf{3.35}&\textbf{3.35}\\
(D)&\textbf{1.8}&\textbf{1.8}\\
\end{tabular}
\item None of the items was reassigned.
\end{enumerate}
\end{enumerate}
\item Consider the \texttt{NCI60} data. This is a cancer cell line microarray data, which consists of 6830 gene expression measurements on 64 cancer cell lines. Each row is a cancer cell line which is labeled with a cancer type (the first column of the data). In what follows, use Euclidean distance on the unstandardized variables.
<<>>=
nci60 <- read.csv("NCI60.csv")
nci60data <- nci60[2:ncol(nci60)]
nci60data <- scale(nci60data, center = TRUE, scale = TRUE)
rownames(nci60data) <- make.unique(as.character(nci60[,1]))
nci60dist <- dist(nci60data, method = "euclidean")
@
\begin{enumerate}
\item Carry out complete, average and single linkage hierarchical clustering of the cancer cell line. Report a dendrogram with labels. Which linkage do you like best? Why?
<<fig=TRUE, height=5>>=
par(mar=c(2,1,1,6), mfrow=c(1,3),mgp=c(2,1,0), cex=0.3)
plotDendogram <- function(x){
  result <- hclust(nci60dist, method = tolower(x))
  par(cex = 0.3)
  plot(as.dendrogram(result), cex=0.2, horiz = TRUE, 
       xlab="", ylab="", main="", 
       sub="", axes=FALSE, xlim = c(160,0))
  abline(v=(mean(result$height[nrow(result$merge)-c(2,3)])), 
         lty = 2, 
         col="red")
  par(cex=0.5)
  title(xlab="xlab", ylab="ylab", main=x)
  axis(1)
}
plotDendogram("Complete")
plotDendogram("Average")
plotDendogram("Single")
@
\item Following we will only use the result getting from complete method. Plot the cut on the dendrogram that produces four clusters and compare the result with the original labels. \textit{Labels are not organized in the same order}. Find one type of cancer fall in one cluster and find one type of cancer spread out over three different clusters. \textit{Breast cancer is present in clusters 2, 1 and 4. Melanoma is only present in cluster 1}
<<>>=
cutree(hclust(nci60dist, method = "complete"),4)[sort(rownames(nci60data))]
@
\item Carry out K-means clustering of the cancer cell lines, with K = 4. Plot the first two principal components and color-code by cluster membership. How do the K-means results compare with the hierarchical clustering results? (Using \texttt{set.seed(2)} before your K-means algorithm and remember scale your data before doing PCA.) \textit{They are not identical but quite similar, Breast cancer is still present in 3 clusters and Melanoma belong to just one.}
<<fig=TRUE, height=2>>=
pca_nci60 <- eigen(var(nci60data))
pc_nci60 <- (nci60data %*% pca_nci60$vectors)[,1:2]
set.seed(2)
KM <- kmeans(pc_nci60,4)
KM$cluster[sort(rownames(nci60data))]
par(mar=c(3,3,1,1))
plot(pc_nci60, xlab = "PC1", ylab = "PC2")
points(pc_nci60, col = KM$cluster, pch = 20, cex = 0.8)
@
\end{enumerate}
\end{enumerate}
\end{document}